I"“#<aside class="sidebar__right">
<nav class="toc">
    <header><h4 class="nav__title"><i class="fa fa-file-text"></i> Table</h4></header>
<ul class="toc__menu" id="markdown-toc">
  <li><a href="#install" id="markdown-toc-install">Install</a></li>
  <li><a href="#import" id="markdown-toc-import">Import</a></li>
  <li><a href="#tutorial" id="markdown-toc-tutorial">Tutorial</a></li>
  <li><a href="#nn-module" id="markdown-toc-nn-module">nn module</a></li>
  <li><a href="#note" id="markdown-toc-note">Note</a></li>
</ul>

  </nav>
</aside>

<h1 id="install">Install</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conda</span> <span class="n">install</span> <span class="n">pytorch</span> <span class="n">torchvision</span> <span class="o">-</span><span class="n">c</span> <span class="n">soumith</span>
</code></pre></div></div>

<h1 id="import">Import</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
</code></pre></div></div>

<h1 id="tutorial">Tutorial</h1>

<p>http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html</p>

<p>http://pytorch.org/tutorials/</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.00000e-44 *
  0.0000  0.0000  0.0000
  0.0000  1.6816  0.0000
  0.0000  0.0000  0.0000
  0.0000  0.0000  0.0000
  0.0000  0.0000  0.0000
[torch.FloatTensor of size 5x3]
</code></pre></div></div>

<h1 id="nn-module">nn module</h1>

<p>http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#nn-module</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="c1"># N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.
</span><span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Use the nn package to define our model as a sequence of layers. nn.Sequential
# is a Module which contains other Modules, and applies them in sequence to
# produce its output. Each Linear Module computes output from input using a
# linear function, and holds internal Variables for its weight and bias.
</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># The nn package also contains definitions of popular loss functions; in this
# case we will use Mean Squared Error (MSE) as our loss function.
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># Forward pass: compute predicted y by passing x to the model. Module objects
</span>    <span class="c1"># override the __call__ operator so you can call them like functions. When
</span>    <span class="c1"># doing so you pass a Variable of input data to the Module and it produces
</span>    <span class="c1"># a Variable of output data.
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Compute and print loss. We pass Variables containing the predicted and true
</span>    <span class="c1"># values of y, and the loss function returns a Variable containing the
</span>    <span class="c1"># loss.
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">t</span><span class="o">%</span><span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Zero the gradients before running the backward pass.
</span>    <span class="n">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Backward pass: compute gradient of the loss with respect to all the learnable
</span>    <span class="c1"># parameters of the model. Internally, the parameters of each Module are stored
</span>    <span class="c1"># in Variables with requires_grad=True, so this call will compute gradients for
</span>    <span class="c1"># all learnable parameters in the model.
</span>    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update the weights using gradient descent. Each parameter is a Variable, so
</span>    <span class="c1"># we can access its data and gradients like we did before.
</span>    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0, 717.2719116210938)
(50, 35.097198486328125)
(100, 1.8821511268615723)
(150, 0.1728428155183792)
(200, 0.02194761298596859)
(250, 0.0034840735606849194)
(300, 0.0006572074489668012)
(350, 0.00014404028479475528)
(400, 3.580378324841149e-05)
(450, 9.810625670070294e-06)
</code></pre></div></div>

<h1 id="note">Note</h1>
:ET